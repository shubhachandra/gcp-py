Hereâ€™s a clear and structured response that addresses each of your questions, tailored to your scenario involving routing updates in Google Cloud across SDLCs in the ad-ENT environment:

â¸»

1. What are the benefits of implementing this change and impact if this change is not implemented?

Benefits:
	â€¢	Ensures consistent routing behavior across all SDLCs (sandbox, nonprod, prod, and core).
	â€¢	Enables core SDLC to function as a transit network, allowing traffic between other SDLCs.
	â€¢	Prevents asymmetric routing and blackhole issues by keeping all routing tables in sync.
	â€¢	Improves operational hygiene by removing outdated or inconsistent routes.
	â€¢	Sets up a scalable and manageable routing structure across the environment.

Impact if not implemented:
	â€¢	Risk of network segmentation, where environments may not reach each other due to stale or incorrect routes.
	â€¢	Transit traffic via core may break, leading to outages or degraded service for dependent applications.
	â€¢	Security and compliance issues from unintentional exposure or blocked traffic.
	â€¢	Increased manual overhead to maintain workarounds for broken network paths.

â¸»

2. What customer or client could be impacted in the blast radius if changes do not go as planned?
	â€¢	Internal business applications relying on communication across SDLCs could break, especially services in sandbox/nonprod needing access to shared tools or core services.
	â€¢	Critical production workloads that rely on backend services in the core may fail, potentially affecting external-facing customer systems.
	â€¢	Shared services such as logging, monitoring, IAM proxies, or service accounts hosted in the core may become unreachable.
	â€¢	Developer and QA teams may lose access to test or deploy if their VPCs become isolated.

Blast Radius Summary: High â€” due to the transit dependency on the core SDLC for all network traffic.

â¸»

3. Describe post-change validation and checkpoint plan (CI is related to this change)?

Post-change validation plan:
	â€¢	Automated health checks for each VPC to validate reachability to critical endpoints (e.g., DNS, GKE clusters, Bastion).
	â€¢	Ping and trace route tests across SDLCs to validate routing flow (e.g., sandbox â†’ core, nonprod â†’ prod).
	â€¢	Validate Cloud Interconnect (CI) or VPN BGP routes if usedâ€”ensure prefixes are advertised and accepted correctly.
	â€¢	Confirm that all firewall rules and IAM bindings tied to network tags still allow intended traffic.

Checkpoints:
	â€¢	Run Terraform plan & apply with -target to staged resources if needed.
	â€¢	Snapshot and store pre-change routing tables for rollback reference.
	â€¢	Run post-apply checks and document findings in a shared Change Management document.

â¸»

4. Describe communication to business operations and Command Centre?
	â€¢	Pre-change notification: Send communication 24-48 hours prior detailing:
	â€¢	Change window
	â€¢	SDLCs and services impacted
	â€¢	Expected behavior and test plan
	â€¢	Command Center Alert: Real-time update during the change via Slack/email, using a dedicated bridge or ticketing tool like ServiceNow/JIRA.
	â€¢	Status updates: At key phases â€” Start, Mid-checkpoint, and Completion.
	â€¢	Post-change summary: Include test results, rollback plan status, and links to logs/monitoring dashboards.

â¸»

5. Why pre-testing is not possible?
	â€¢	Core SDLC acts as a transit, and partial updates create incomplete or broken routing paths.
	â€¢	Staggering changes across SDLCs can invalidate test results due to dependency on fully consistent routing across environments.
	â€¢	Many systems (e.g., CI, shared logging, security controls) rely on inter-SDLC connectivity, which cannot be validated in isolation.
	â€¢	QA-ENT testing already validated the logic; this environment is representative of the final configuration.

â¸»

6. Risk Management Statement

While the routing changes are complex and impact multiple SDLCs, the change is necessary to ensure a stable and scalable network. Risks are mitigated through prior validation in QA-ENT, snapshotting current state, real-time monitoring, and a well-defined rollback plan. The team is prepared for coordinated, all-at-once deployment to maintain integrity of the core transit layer.

â¸»

Let me know if youâ€™d like this formatted as a change request template or Confluence page.

Thanks, Shubh. Based on the details youâ€™ve provided, hereâ€™s a refined post-change validation checklist and supporting clarification around the specific /18 subnet entries â€” all focused on ensuring the SNI team verifies proper routing from on-prem to GCP.

â¸»

âœ… Post-Change Validation Checklist â€“ SNI Team Responsibility

1. Health Check Ranges (GCP Load Balancers)

Verify the following IP ranges are routable from on-prem to GCP, especially from any appliance that monitors or terminates traffic (like firewalls or health probes):
	â€¢	35.191.0.0/16 â€“ GCP LB health checks (global)
	â€¢	130.211.0.0/22 â€“ GCP LB health checks (legacy)

Validation: Check firewalls, routers, and BGP routes to confirm these ranges are allowed and reachable from on-prem.

â¸»

2. Core Routing Validation â€“ GCP SDLC Transit

Core Ranges (Advertised)

Ensure reachability from on-prem to GCP core SDLC over the following /17 routes:
	â€¢	100.126.128.0/17 â€“ us-central1 core
	â€¢	100.123.128.0/17 â€“ us-east1 core

â¸»

3. Sandbox Core Routing

Advertised Core Routes (Sandbox)

Ensure the following sandbox-specific cores are routed correctly from on-prem:
	â€¢	100.126.0.0/17 â€“ us-central1 sandbox
	â€¢	100.123.0.0/17 â€“ us-east1 sandbox

â¸»

4. /18 Adjustments to Prevent Asymmetric Routing Issues (Trip Routing Fixes)

Due to the local-pref weight in on-prem routers, the /17 blocks are split into /18 subnets to ensure proper preference and route selection.

ðŸ”½ For us-east1 core (sandbox):
	â€¢	100.123.0.0/18 â€“ primary
	â€¢	100.123.64.0/18 â€“ companion split

ðŸ”½ For us-central1 core (sandbox):
	â€¢	100.126.0.0/18 â€“ primary
	â€¢	100.128.64.0/18 â€“ companion split

â¸»

ðŸ”½ For us-east1 nonprod:
	â€¢	100.125.0.0/18 â€“ primary
	â€¢	100.125.64.0/18 â€“ split 1
	â€¢	100.125.128.0/18 â€“ split 2
	â€¢	100.125.192.0/18 â€“ split 3

ðŸ”½ For us-central1 nonprod:
	â€¢	100.124.0.0/18 â€“ primary
	â€¢	100.124.64.0/18 â€“ companion split

â¸»

ðŸ”½ For us-central1 prod:
	â€¢	100.124.128.0/18 â€“ primary
	â€¢	100.124.192.0/18 â€“ companion split

â¸»

ðŸ”½ For us-east1 prod:
	â€¢	100.125.128.0/18 â€“ primary
	â€¢	100.125.192.0/18 â€“ companion split

Validation Action:
Ensure all of these /18 subnets are correctly propagated and visible in:

	â€¢	On-prem BGP route tables
	â€¢	Cloud Router BGP sessions (on GCP)
	â€¢	Any relevant NVA/VPN/Interconnect appliances

â¸»

ðŸ“Œ Additional Notes for SNI Team
	â€¢	These /18s are not arbitrary â€” theyâ€™re specifically used to influence route selection by breaking larger blocks to force preference via GCP vs. on-prem due to how Local Preference (local-pref) is being honored in BGP decisions.
	â€¢	SNI should ensure only intended routes are preferred and symmetric return paths are maintained (no blackholes or misrouted return packets).

â¸»

Would you like this in a Confluence-ready table format or a pre-filled email draft to share with the SNI team?

Sure, hereâ€™s a simpler and more concise version using plain language:

â¸»

What customer or client could be impacted in the blast radius if changes do not go as planned?

If the routing changes do not go as planned, internal applications and teams that rely on communication between environments (like sandbox, nonprod, prod, and core) may face issues. For example:
	â€¢	Applications in one environment may not be able to talk to shared services in the core.
	â€¢	Teams may lose access to shared tools, monitoring systems, or CI/CD pipelines.
	â€¢	Developers and testers might see delays or errors during deployments.

These issues can slow down day-to-day operations or affect how quickly problems are fixed.

â¸»

Risk Mitigation Statement

This is a minor but important routing update. To reduce risk:
	â€¢	We already tested the changes successfully in the QA environment.
	â€¢	All changes are being applied together across environments to avoid partial issues.
	â€¢	We have snapshots of current routes for rollback if needed.
	â€¢	The network and SNI teams are ready to validate immediately after changes are made.

This helps ensure a smooth update with minimal impact.
